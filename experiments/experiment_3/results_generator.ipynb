{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import importlib  \n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "\n",
    "from sklearn.utils import _safe_indexing\n",
    "sys.modules['sklearn.utils.safe_indexing'] = _safe_indexing\n",
    "\n",
    "from pairs_trading_package.clustering import *\n",
    "\n",
    "import pairs_trading_package as lfl\n",
    "from pairs_trading_package.utils import flatten, postfix_keys_to_dict, get_current_time_hash, get_random_hash\n",
    "\n",
    "from pairs_trading_package.pairs_trading_backtester import (\n",
    "    Trader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_merged_analytics_as_dataframe(file_list, filter_results=True):\n",
    "\n",
    "    results_per_period = []\n",
    "\n",
    "    for file in file_list:\n",
    "        backtest_results_ae_df = pd.read_csv(file['filepath'])\n",
    "\n",
    "        working_example_df_ae = backtest_results_ae_df[backtest_results_ae_df['clust_algo'] == file['clust_algo']]\n",
    "        \n",
    "        if filter_results:\n",
    "            working_example_df_ae = working_example_df_ae[working_example_df_ae['distance_measure'] == file['distance_measure']] #54 #70\n",
    "            working_example_df_ae = working_example_df_ae[working_example_df_ae['n_clusters'] == file['n_clusters']] #54 #70\n",
    "\n",
    "        results_per_period.append(working_example_df_ae)\n",
    "\n",
    "    return pd.concat(results_per_period)\n",
    "\n",
    "def get_merged_results_as_dataframe(file_list):\n",
    "\n",
    "    results_per_period = []\n",
    "\n",
    "    for file in file_list:\n",
    "        backtest_results_ae_df = pd.read_csv(meta_data_files[file['split_idx']])\n",
    "\n",
    "        working_example_df_ae = backtest_results_ae_df[backtest_results_ae_df['clust_algo'] == file['clust_algo']]\n",
    "        working_example_df_ae = working_example_df_ae[working_example_df_ae['distance_measure'] == file['distance_measure']]\n",
    "        working_example_df_ae = working_example_df_ae[working_example_df_ae['n_clusters'] == file['n_clusters']]\n",
    "\n",
    "        results_per_period.append(working_example_df_ae)\n",
    "\n",
    "    merged_results = []\n",
    "\n",
    "    for period_zero_sample in results_per_period[0].iterrows():\n",
    "\n",
    "        working_period_one_df_ae = results_per_period[1].dropna()\n",
    "        working_period_one_df_ae = working_period_one_df_ae[working_period_one_df_ae['seed'] == period_zero_sample[1]['seed']]\n",
    "\n",
    "        if len(working_period_one_df_ae) != 0:\n",
    "            working_period_one_df_ae = working_period_one_df_ae.dropna().sample(1)\n",
    "        else: \n",
    "            working_period_one_df_ae = results_per_period[1].dropna().sample(1)\n",
    "\n",
    "        working_period_two_df_ae = results_per_period[2].dropna()\n",
    "        working_period_two_df_ae = working_period_two_df_ae[working_period_two_df_ae['seed'] == period_zero_sample[1]['seed']].dropna()\n",
    "\n",
    "        if len(working_period_two_df_ae) != 0:\n",
    "            working_period_two_df_ae = working_period_two_df_ae.dropna().sample(1)\n",
    "        else: \n",
    "            working_period_two_df_ae = results_per_period[2].dropna().sample(1)\n",
    "\n",
    "        merged_results.append([period_zero_sample[1]['clust_algo'], period_zero_sample[1]['seed'],\n",
    "\n",
    "                               period_zero_sample[1]['portfolio_returns_saved_file_insample'], \n",
    "                               period_zero_sample[1]['portfolio_returns_saved_file_oosample'],\n",
    "\n",
    "                               working_period_one_df_ae['portfolio_returns_saved_file_insample'].values[0], \n",
    "                               working_period_one_df_ae['portfolio_returns_saved_file_oosample'].values[0],\n",
    "\n",
    "                               working_period_two_df_ae['portfolio_returns_saved_file_insample'].values[0], \n",
    "                               working_period_two_df_ae['portfolio_returns_saved_file_oosample'].values[0],\n",
    "                              ])\n",
    "\n",
    "    merged_results_df = pd.DataFrame(merged_results, columns=['clust_algo', 'rand_seed', \n",
    "                           'period_zero_returns_file_insample', 'period_zero_returns_file_oosample',\n",
    "                           'period_one_returns_file_insample', 'period_one_returns_file_oosample',\n",
    "                           'period_two_returns_file_insample', 'period_two_returns_file_oosample',\n",
    "                          ])\n",
    "\n",
    "    return merged_results_df\n",
    "\n",
    "def sample_n_times(N, file_list):\n",
    "    sampled_dfs = []\n",
    "    \n",
    "    for _ in range(N):\n",
    "        sample_df = get_merged_results_as_dataframe(file_list)\n",
    "        sampled_dfs.append(sample_df)\n",
    "        \n",
    "    return pd.concat(sampled_dfs)\n",
    "\n",
    "def get_collated_returns_as_dataframe(merged_results_df, sample_period='oosample'):\n",
    "    collated_dfs = []\n",
    "\n",
    "    for full_period_sample in merged_results_df.iterrows():\n",
    "        per_zero = pd.read_csv('../data_folder/results_visualizations/backtest_results/return_series/' + full_period_sample[1]['period_zero_returns_file_' + sample_period])\n",
    "        per_one = pd.read_csv('../data_folder/results_visualizations/backtest_results/return_series/' + full_period_sample[1]['period_one_returns_file_' + sample_period])\n",
    "        per_two = pd.read_csv('../data_folder/results_visualizations/backtest_results/return_series/' + full_period_sample[1]['period_two_returns_file_' + sample_period])\n",
    "\n",
    "        collated_df = pd.concat([per_zero, per_one, per_two])\n",
    "        collated_df['Date'] = pd.to_datetime(collated_df['Date'])\n",
    "        collated_df.set_index('Date', inplace=True, drop=True)\n",
    "\n",
    "        collated_dfs.append(collated_df)\n",
    "        \n",
    "    return collated_dfs\n",
    "\n",
    "\n",
    "def get_sharpe_distribution_from_collated_returns(collated_dfs, rf_rate=0.01):\n",
    "    \n",
    "    annualized_ret = pd.concat(collated_dfs, axis=1).mean()*252\n",
    "    vol = pd.concat(collated_dfs, axis=1).std()*np.sqrt(252)\n",
    "\n",
    "    rf_daily = (1+rf_rate)**(1/252)-1\n",
    "\n",
    "    sharpe_ratio_assuming_iid = (annualized_ret-rf_daily) /vol\n",
    "    \n",
    "    return sharpe_ratio_assuming_iid\n",
    "\n",
    "\n",
    "def get_mdd_distribution_from_collated_returns(collated_dfs, rf_rate=0.01):\n",
    "    trader_obj = Trader()\n",
    "    \n",
    "    cum_rets = pd.concat(collated_dfs, axis=1).cumsum()\n",
    "\n",
    "    mdd_dist = []\n",
    "    for cr in range(len(cum_rets.columns)):\n",
    "        mdd_dist.append( trader_obj.calculate_maximum_drawdown(1+cum_rets.iloc[:, cr]*1, False)[0] )\n",
    "    \n",
    "    return np.array(mdd_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_data_files = ['../data_folder/results_visualizations/backtest_results/a0ffcabf03d9c1320e60b09556b7dd0a_ae.csv', \n",
    "                   '../data_folder/results_visualizations/backtest_results/fin_bf672f94b8548e9bb32e9a6a20463a07_ae.csv', \n",
    "                   '../data_folder/results_visualizations/backtest_results/38d975761f1e86d3b2002e92d2f83f7b_ae_.csv']\n",
    "\n",
    "\n",
    "meta_data_splits = [ \n",
    "    ['2012 - 2016'],\n",
    "    ['2013 - 2017'],\n",
    "    ['2014 - 2018'] \n",
    "]\n",
    "\n",
    "cluster_scores = [{'split_idx': 0, 'clust_algo': 'kmeans', 'distance_measure': 'euclidean', 'n_clusters': 30},\n",
    "                  {'split_idx': 1, 'clust_algo': 'kmeans', 'distance_measure': 'euclidean', 'n_clusters': 30},\n",
    "                  {'split_idx': 2, 'clust_algo': 'kmeans', 'distance_measure': 'euclidean', 'n_clusters': 30}]\n",
    "\n",
    "\n",
    "clust_data_list = []\n",
    "\n",
    "for first_pass_idx in range(3):\n",
    "\n",
    "    cs = cluster_scores[first_pass_idx]\n",
    "\n",
    "    current_clust_data = {}\n",
    "    current_clust_data['split_idx'] = meta_data_splits[cs['split_idx']][0] \n",
    "    current_clust_data['clust_algo'] = cs['clust_algo']\n",
    "    current_clust_data['distance_measure'] = cs['distance_measure']\n",
    "\n",
    "    working_df = pd.read_csv(meta_data_files[cs['split_idx']])\n",
    "    working_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    working_df.reset_index(drop=True, inplace=True)\n",
    "    working_df.drop_duplicates(inplace=True)\n",
    "    working_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    working_df.dropna(inplace=True)\n",
    "\n",
    "    first_pass_df = working_df[working_df['clust_algo'] == cs['clust_algo']]\n",
    "    second_pass_df = first_pass_df[first_pass_df['distance_measure'] == cs['distance_measure']]\n",
    "\n",
    "    cols_to_select = ['annual_sharpe_ratio_iid_insample', 'avg_total_roi_insample', 'max_dd_insample', \n",
    "                                                                        'annual_sharpe_ratio_iid_oosample', 'avg_total_roi_oosample', 'max_dd_oosample']\n",
    "    third_pass_df = second_pass_df[second_pass_df['n_clusters'] == cs['n_clusters']].loc[:, cols_to_select]\n",
    "\n",
    "\n",
    "    if len(third_pass_df) == 0: \n",
    "        nearest_available = second_pass_df['n_clusters'].values[np.argmin(np.abs(second_pass_df['n_clusters'].values - cs['n_clusters']))]\n",
    "\n",
    "        third_pass_df = second_pass_df[second_pass_df['n_clusters'] == nearest_available].loc[:, cols_to_select]\n",
    "        cluster_scores[first_pass_idx+second_pass_idx]['n_clusters'] = nearest_available\n",
    "\n",
    "    third_pass_df_means = np.around(third_pass_df.mean(), 2) \n",
    "    third_pass_df_stds = np.around(third_pass_df.std(), 2)\n",
    "\n",
    "    for col_to_select in cols_to_select:\n",
    "        current_clust_data[col_to_select] = str(third_pass_df_means[col_to_select]) + ' (' + str(third_pass_df_stds[col_to_select]) + ')'\n",
    "\n",
    "    clust_data_list.append(current_clust_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(clust_data_list).to_csv('./pretty_results/per_period_performance_table_ae.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_data_list_full_period = []\n",
    "\n",
    "\n",
    "intermediate_cluster_scores_array = []\n",
    "    \n",
    "for first_pass_idx in range(3):\n",
    "    cs = cluster_scores[first_pass_idx]\n",
    "    intermediate_cluster_scores_array.append(cs)\n",
    "        \n",
    "current_clust_data_full = {}\n",
    "current_clust_data_full['clust_algo'] = cs['clust_algo']\n",
    "current_clust_data_full['distance_measure'] = cs['distance_measure']\n",
    "\n",
    "merged_results_pca_df = sample_n_times(2, intermediate_cluster_scores_array)\n",
    "\n",
    "pca_returns_is_collated_df = get_collated_returns_as_dataframe(merged_results_pca_df, 'insample')\n",
    "pca_returns_oos_collated_df = get_collated_returns_as_dataframe(merged_results_pca_df, 'oosample')\n",
    "\n",
    "pca_is_cumrets_results = pd.concat(pca_returns_is_collated_df, axis=1).mean()*252#.iloc[-1]\n",
    "pca_oos_cumrets_results = pd.concat(pca_returns_oos_collated_df, axis=1).mean()*252#.iloc[-1]\n",
    "\n",
    "current_clust_data_full['insample_roi'] = str(np.round(pca_is_cumrets_results.mean()*100, 2)) + ' (' + str(np.round(pca_is_cumrets_results.std()*100, 2)) + ')'\n",
    "current_clust_data_full['oosample_roi'] = str(np.round(pca_oos_cumrets_results.mean()*100, 2)) + ' (' + str(np.round(pca_oos_cumrets_results.std()*100, 2)) + ')'\n",
    "\n",
    "pca_is_sharpe_results = get_sharpe_distribution_from_collated_returns(pca_returns_is_collated_df)\n",
    "pca_oos_sharpe_results = get_sharpe_distribution_from_collated_returns(pca_returns_oos_collated_df)\n",
    "\n",
    "current_clust_data_full['insample_sharpe'] = str(np.round(pca_is_sharpe_results.mean(), 2)) + ' (' + str(np.round(pca_is_sharpe_results.std(), 2)) + ')'\n",
    "current_clust_data_full['oosample_sharpe'] = str(np.round(pca_oos_sharpe_results.mean(), 2)) + ' (' + str(np.round(pca_oos_sharpe_results.std(), 2)) + ')'\n",
    "\n",
    "pca_is_mdd_results = get_mdd_distribution_from_collated_returns(pca_returns_is_collated_df)\n",
    "pca_oos_mdd_results = get_mdd_distribution_from_collated_returns(pca_returns_oos_collated_df)\n",
    "\n",
    "current_clust_data_full['insample_mdd'] = str(np.round(pca_is_mdd_results.mean(), 2)) + ' (' + str(np.round(pca_is_mdd_results.std(), 2)) + ')'\n",
    "current_clust_data_full['oosample_mdd'] = str(np.round(pca_oos_mdd_results.mean(), 2)) + ' (' + str(np.round(pca_oos_mdd_results.std(), 2)) + ')'\n",
    "\n",
    "clust_data_list_full_period.append(current_clust_data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clust_data_list_full_period).to_csv('./pretty_results/full_period_performance_table_ae.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
