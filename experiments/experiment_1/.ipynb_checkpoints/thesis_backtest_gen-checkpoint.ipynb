{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641192932202,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "T3cWXLMqbPa9"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4226,
     "status": "ok",
     "timestamp": 1641192936424,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "zCAwmrccT6q5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, OPTICS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "\n",
    "from sklearn.utils import _safe_indexing\n",
    "sys.modules['sklearn.utils.safe_indexing'] = _safe_indexing\n",
    "\n",
    "from pairs_trading_package.clustering import *\n",
    "\n",
    "import pairs_trading_package as lfl\n",
    "from pairs_trading_package.utils import flatten, postfix_keys_to_dict, get_current_time_hash, get_random_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1641192937217,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "Ntm5osuqT6uT"
   },
   "outputs": [],
   "source": [
    "ticker_path = \"../data_folder/original/ticker_segment_dict.pickle\"\n",
    "with open(ticker_path, 'rb') as handle:\n",
    "    ticker_segment_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5884,
     "status": "ok",
     "timestamp": 1641192943097,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "W921dxgvT60k",
    "outputId": "cbc6db35-c32a-48d9-f7e2-b69e24746868"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pairs_trading_package.pairs_trading_backtester import (\n",
    "    SeriesAnalyser, DataProcessor, MPSeriesAnalyser, MPTrader, Trader\n",
    ")\n",
    "\n",
    "series_analyser = SeriesAnalyser()\n",
    "data_processor = DataProcessor()\n",
    "\n",
    "# load etf metadata\n",
    "etfs, etfs_unique, tickers = data_processor.read_ticker_excel(path='../data_folder/original/commodity_ETFs_long_updated.xlsx')\n",
    "\n",
    "SPLIT_IDX = 1\n",
    "\n",
    "pickle_folders = ['2012-2016', '2013-2017', '2014-2018']\n",
    "\n",
    "# train split, test split, train_val_split\n",
    "splits = [ [('01-01-2012', '31-12-2014'), ('01-01-2015', '31-12-2015'), '2014-01-01'],\n",
    "[('01-01-2013', '31-12-2015'), ('01-01-2016', '31-12-2016'), '2015-01-01'],\n",
    "[('01-01-2014', '31-12-2016'), ('01-01-2017', '31-12-2017'), '2016-01-01'] ]\n",
    "\n",
    "subsample = 2500\n",
    "\n",
    "min_half_life = 78 # number of points in a day\n",
    "\n",
    "max_half_life = 20000 #~number of points in a year: 78*252\n",
    "\n",
    "# intraday\n",
    "n_years_val = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1641192943485,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "kpA8URUTAXbW"
   },
   "outputs": [],
   "source": [
    "\n",
    "naming_scheme = dict({'kmeans': {'no_clusters': 'n_clusters', 'algo': 'kmeans', 'distance': 'euclidean'},\n",
    "                      'agglomerative': {'no_clusters': 'n_clusters', 'algo': 'linkage', 'distance': 'affinity'},\n",
    "                      'spectral': {'no_clusters': 'n_clusters', 'algo': 'spectral', 'distance': 'affinity'},\n",
    "                      'optics': {'no_clusters': 'min_samples', 'algo': 'optics', 'distance': 'metric'},                      \n",
    "                      })\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1641192943490,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "V6pNgpEIwFLo"
   },
   "outputs": [],
   "source": [
    "from pairs_trading_package.multiple_hypothesis_corrections.fdr import abh\n",
    "\n",
    "def correct_pairs_fdr(pairs_unsupervised, pvals, q=0.1):\n",
    "\n",
    "    significant_pairs_unsupervised = []\n",
    "    \n",
    "    for pair in pairs_unsupervised:\n",
    "        if pair[2]['p_value'] in pvals[abh(pvals, q=q)]:\n",
    "            significant_pairs_unsupervised.append(pair)\n",
    "            \n",
    "    return significant_pairs_unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1641192948807,
     "user": {
      "displayName": "Aaron Debrincat",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06157220713259326892"
     },
     "user_tz": -60
    },
    "id": "o3lEkXfks5A8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def save_portfolio_returns(performance_df, number_of_pairs, folder='./backtest_results/portfolio_return_series/'):\n",
    "    \n",
    "    total_account_balance = performance_df[0][1]['account_balance'].resample('D').last().dropna()\n",
    "    portfolio_returns = total_account_balance.pct_change().fillna(0)\n",
    "    for index in range(1, len(number_of_pairs)):\n",
    "        pair_balance = performance_df[index][1]['account_balance'].resample('D').last().dropna()\n",
    "        portfolio_returns = pd.concat([portfolio_returns, pair_balance.pct_change().fillna(0)], axis=1)\n",
    "\n",
    "    weights = np.array([1 / len(number_of_pairs)] * len(number_of_pairs))\n",
    "    \n",
    "    weighted_portfolio_returns = pd.Series(np.dot(portfolio_returns.fillna(0), weights), index=portfolio_returns.index)\n",
    "\n",
    "    return_file_name = get_random_hash() + '.csv'\n",
    "    \n",
    "    weighted_portfolio_returns.to_csv(folder + return_file_name)\n",
    "    \n",
    "    return return_file_name\n",
    "\n",
    "\n",
    "def save_spread_returns(performance_df, folder='./backtest_results/spread_series/'):\n",
    "\n",
    "    spreads_series = []\n",
    "\n",
    "    for ptt_spread in performance_df:\n",
    "        spread_name = ptt_spread[0][0] + '_' + ptt_spread[0][1]\n",
    "        spread_series = ptt_spread[1]['norm_spread'].resample('D').last().dropna()\n",
    "        spread_series.name = spread_name\n",
    "        spreads_series.append(spread_series)\n",
    "\n",
    "    spreads_series_df = pd.concat(spreads_series, axis=1)\n",
    "\n",
    "    return_file_name = get_random_hash() + '.csv'\n",
    "\n",
    "    spreads_series_df.to_csv(folder + return_file_name)\n",
    "\n",
    "    return return_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_VAL_THRESHOLD = 0.10\n",
    "Q_VAL_THRESHOLD = 0.10\n",
    "HURST_THRESHOLD = 0.5\n",
    "ZERO_CROSSINGS = 12\n",
    "MHC_METHOD = 'Benjamini Hochberg'\n",
    "DIM_RED_METHOD = 'PCA'\n",
    "    \n",
    "mp_series_analyser = MPSeriesAnalyser(109)\n",
    "   \n",
    "dict_tobe_filled = { \n",
    "    # Describing Dates \n",
    "    'train_period_start': '', 'train_period_end': '',  \n",
    "    'validation_period': '', \n",
    "    'test_period_start': '', 'test_period_end': '', \n",
    "\n",
    "    # Describing the clustering parameters used\n",
    "    'seed': 0, 'n_clusters': 0, 'clust_algo': '', 'distance_measure': '', \n",
    "\n",
    "    # Describing the clustering achieved\n",
    "    'infomax_hofs': 0, 'infomax_hofk': 0, 'no_found_pairs_from_clustering': 0, \n",
    "\n",
    "    # Describing the dimensionality reduction  technique and resulting product\n",
    "    'n_principal_components': 0, 'explained_variance': 0, \n",
    "\n",
    "    # Statistical thresholds used for filtering of FP\n",
    "    'qvalue_threshold': Q_VAL_THRESHOLD, 'multiple_hypothesis_correction_method': MHC_METHOD, \n",
    "    'pvalue_threshold': P_VAL_THRESHOLD, 'hurst_threshold': HURST_THRESHOLD, \n",
    "    'min_zero_crossings': ZERO_CROSSINGS,\n",
    "    \n",
    "    # Insample trading results\n",
    "    'n_pairs_insample': 0, 'annual_sharpe_ratio_iid_insample': 0, 'auto_corr_insample': 0, \n",
    "    'daily_sharpe_ratio_insample': 0, 'portfolio_vol_insample': 0, 'avg_total_roi_insample': 0, \n",
    "    'avg_annual_roi_insample': 0, 'total_trades_insample': 0, 'positive_trades_insample': 0,\n",
    "    'negative_trades_insample': 0, 'pct_positive_trades_per_pair_insample': 0, 'pct_pairs_with_positive_results_insample': 0, \n",
    "    'max_dd_insample': 0, 'max_dd_duration_insample': 0, 'total_dd_duration_insample': 0, 'avg_half_life_insample': 0, \n",
    "    'avg_hurst_exponent_insample': 0, 'count_positive_trades_dist_insample': 0, 'mean_positive_trades_dist_insample': 0, \n",
    "    'std_positive_trades_dist_insample': 0, 'min_positive_trades_dist_insample': 0, '25%_positive_trades_dist_insample': 0, \n",
    "    '50%_positive_trades_dist_insample': 0, '75%_positive_trades_dist_insample': 0, 'max_positive_trades_dist_insample': 0, \n",
    "    'count_negative_trades_dist_insample': 0, 'mean_negative_trades_dist_insample': 0, 'std_negative_trades_dist_insample': 0, \n",
    "    'min_negative_trades_dist_insample': 0, '25%_negative_trades_dist_insample': 0, '50%_negative_trades_dist_insample': 0, \n",
    "    '75%_negative_trades_dist_insample': 0, 'max_negative_trades_dist_insample': 0, 'count_pairs_sharpe_dist_insample': 0, \n",
    "    'mean_pairs_sharpe_dist_insample': 0, 'std_pairs_sharpe_dist_insample': 0, 'min_pairs_sharpe_dist_insample': 0, \n",
    "    '25%_pairs_sharpe_dist_insample': 0, '50%_pairs_sharpe_dist_insample': 0, '75%_pairs_sharpe_dist_insample': 0, \n",
    "    'max_pairs_sharpe_dist_insample': 0, 'portfolio_returns_saved_file_insample': '', 'spreads_saved_file_insample': '',\n",
    "    \n",
    "    # Out of Sample trading results\n",
    "    'n_pairs_oosample': 0, 'annual_sharpe_ratio_iid_oosample': 0, 'auto_corr_oosample': 0, \n",
    "    'daily_sharpe_ratio_oosample': 0, 'portfolio_vol_oosample': 0, 'avg_total_roi_oosample': 0, 'avg_annual_roi_oosample': 0,\n",
    "    'total_trades_oosample': 0, 'positive_trades_oosample': 0, 'negative_trades_oosample': 0, 'pct_positive_trades_per_pair_oosample': 0,\n",
    "    'pct_pairs_with_positive_results_oosample': 0, 'max_dd_oosample': 0, 'max_dd_duration_oosample': 0, 'total_dd_duration_oosample': 0, \n",
    "    'avg_half_life_oosample': 0, 'avg_hurst_exponent_oosample': 0, 'count_positive_trades_dist_oosample': 0, 'mean_positive_trades_dist_oosample': 0, \n",
    "    'std_positive_trades_dist_oosample': 0, 'min_positive_trades_dist_oosample': 0, '25%_positive_trades_dist_oosample': 0, \n",
    "    '50%_positive_trades_dist_oosample': 0, '75%_positive_trades_dist_oosample': 0, 'max_positive_trades_dist_oosample': 0, \n",
    "    'count_negative_trades_dist_oosample': 0, 'mean_negative_trades_dist_oosample': 0, 'std_negative_trades_dist_oosample': 0,\n",
    "    'min_negative_trades_dist_oosample': 0, '25%_negative_trades_dist_oosample': 0, '50%_negative_trades_dist_oosample': 0, \n",
    "    '75%_negative_trades_dist_oosample': 0, 'max_negative_trades_dist_oosample': 0, 'count_pairs_sharpe_dist_oosample': 0, \n",
    "    'mean_pairs_sharpe_dist_oosample': 0, 'std_pairs_sharpe_dist_oosample': 0, 'min_pairs_sharpe_dist_oosample': 0, '25%_pairs_sharpe_dist_oosample': 0, \n",
    "    '50%_pairs_sharpe_dist_oosample': 0, '75%_pairs_sharpe_dist_oosample': 0, 'max_pairs_sharpe_dist_oosample': 0,\n",
    "    'portfolio_returns_saved_file_oosample': '', 'spreads_saved_file_oosample': '',\n",
    "    \n",
    "    # Coint Test distribution statistics\n",
    "    'count_coint_pvals_dist': 0, 'mean_coint_pvals_dist': 0, 'std_coint_pvals_dist': 0, 'min_coint_pvals_dist': 0, \n",
    "    '25%_coint_pvals_dist': 0, '50%_coint_pvals_dist': 0, '75%_coint_pvals_dist': 0, 'max_coint_pvals_dist': 0\n",
    "} \n",
    "\n",
    "ex_args = dict({'min_half_life': min_half_life, 'max_half_life': max_half_life, 'min_zero_crossings': ZERO_CROSSINGS,\n",
    "                'p_value_threshold': P_VAL_THRESHOLD, 'hurst_threshold': HURST_THRESHOLD, 'subsample': subsample})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_frame = pd.DataFrame()\n",
    "\n",
    "for SPLIT_IDX in range(0, len(splits)):\n",
    "    \n",
    "    pair_results_cache = []\n",
    "    \n",
    "    for N_PRIN_COMPONENTS in [25]:\n",
    "        \n",
    "        for rand_seed in [109, 112, 1327, 8222, 6985]:\n",
    "            \n",
    "            mp_series_analyser = MPSeriesAnalyser(rand_seed)\n",
    "\n",
    "            if pd.to_datetime(splits[SPLIT_IDX][0][0]) < pd.to_datetime('2012-01-01'):\n",
    "                df_prices = pd.read_pickle('../data_folder/original/commodity_ETFs_intraday_interpolated_screened_no_outliers.pickle') \n",
    "            else:\n",
    "                df_prices = pd.read_pickle('../data_folder/original/commodity_ETFs_from_2014_complete.pickle')\n",
    "\n",
    "            # split data in training and test\n",
    "            df_prices_train, df_prices_test = data_processor.split_data(df_prices,\n",
    "                                                                      splits[SPLIT_IDX][0],\n",
    "                                                                      splits[SPLIT_IDX][1],\n",
    "                                                                      remove_nan=True)\n",
    "            \n",
    "            mp_series_analyser.df_prices_train = df_prices_train\n",
    "            mp_series_analyser.df_prices_test = df_prices_test\n",
    "\n",
    "            train_val_split = splits[SPLIT_IDX][2]\n",
    "\n",
    "            # intraday\n",
    "            n_years_val = round(len(df_prices_train[train_val_split:])/(240*78))\n",
    "\n",
    "            df_returns = data_processor.get_return_series(df_prices_train)\n",
    "\n",
    "            del df_prices\n",
    "\n",
    "            full_arg_templates = generate_clustering_arg_templates(2, len(df_returns.columns))\n",
    "\n",
    "            for alg_templ in full_arg_templates.keys():\n",
    "                for templ in full_arg_templates[alg_templ]:\n",
    "                    no_clusters_param = naming_scheme[alg_templ]['no_clusters']\n",
    "                    algo_param = naming_scheme[alg_templ]['algo']\n",
    "                    distance_param = naming_scheme[alg_templ]['distance']\n",
    "\n",
    "                    if no_clusters_param in templ:\n",
    "                        N_CLUSTERS = templ[no_clusters_param]\n",
    "                    else:\n",
    "                        N_CLUSTERS = no_clusters_param\n",
    "\n",
    "                    if algo_param in templ:\n",
    "                        CLUST_ALGO = templ[algo_param]\n",
    "                    else:\n",
    "                        CLUST_ALGO = algo_param\n",
    "\n",
    "                    if distance_param in templ:\n",
    "                        DISTANCE = templ[distance_param]\n",
    "                    else:\n",
    "                        DISTANCE = distance_param\n",
    "\n",
    "\n",
    "                    working_analytic_dict = dict_tobe_filled.copy()\n",
    "\n",
    "                    working_analytic_dict['seed'] = rand_seed\n",
    "                    working_analytic_dict['train_period_start'] = splits[SPLIT_IDX][0][0]\n",
    "                    working_analytic_dict['train_period_end'] = splits[SPLIT_IDX][0][1]\n",
    "                    working_analytic_dict['validation_period'] = splits[SPLIT_IDX][2] \n",
    "                    working_analytic_dict['test_period_start'] = splits[SPLIT_IDX][1][0] \n",
    "                    working_analytic_dict['test_period_end'] = splits[SPLIT_IDX][1][1]\n",
    "\n",
    "\n",
    "                    if DIM_RED_METHOD == 'PCA':\n",
    "\n",
    "                        X, explained_variance = mp_series_analyser.apply_PCA(N_PRIN_COMPONENTS, df_returns, ignore_first_eigenvector=False)\n",
    "\n",
    "                        working_analytic_dict['dimensionality_reduction_method'] = 'PCA'\n",
    "                        working_analytic_dict['n_principal_components'] = N_PRIN_COMPONENTS\n",
    "                        working_analytic_dict['explained_variance'] = np.sum(explained_variance) \n",
    "\n",
    "#                     elif DIM_RED_METHOD == 'AutoEncoder':\n",
    "\n",
    "#                         X = pd.read_csv('../notebooks/test-strategy/thesis/autoencoder_work/ae_exhaust/embedding_sets/' + str(SPLIT_IDX) + '.csv').iloc[:, 1:]\n",
    "\n",
    "#                         working_analytic_dict['dimensionality_reduction_method'] = 'AutoEncoder'\n",
    "#                         working_analytic_dict['n_principal_components'] = 10\n",
    "#                         working_analytic_dict['explained_variance'] = -1 \n",
    "                        \n",
    "\n",
    "                    if CLUST_ALGO == 'spectral':\n",
    "                        # spectral\n",
    "                        clustered_series_all, clustered_series, counts, assigned_labels = mp_series_analyser.apply_clustering_algo('spectral', pd.DataFrame(X), df_returns.columns, templ)\n",
    "                    elif (CLUST_ALGO == 'agglomerative') or (CLUST_ALGO == 'single') or (CLUST_ALGO == 'complete') or (CLUST_ALGO == 'average'):\n",
    "                        # agglomerative\n",
    "                        clustered_series_all, clustered_series, counts, assigned_labels = mp_series_analyser.apply_clustering_algo('agglomerative', pd.DataFrame(X), df_returns.columns, templ)\n",
    "                    elif CLUST_ALGO == 'kmeans':\n",
    "                        # kmeans\n",
    "                        clustered_series_all, clustered_series, counts, assigned_labels = mp_series_analyser.apply_clustering_algo('kmeans', pd.DataFrame(X), df_returns.columns, templ)\n",
    "                    elif CLUST_ALGO == 'optics':\n",
    "                        # optics\n",
    "                        clustered_series_all, clustered_series, counts, assigned_labels = mp_series_analyser.apply_clustering_algo('optics', pd.DataFrame(X), df_returns.columns, templ)\n",
    "                        N_CLUSTERS = clustered_series.values.max()\n",
    "                    elif CLUST_ALGO == 'dbscan':\n",
    "                        # dbscan \n",
    "                        clustered_series_all, clustered_series, counts, assigned_labels = mp_series_analyser.apply_clustering_algo('dbscan', pd.DataFrame(X), df_returns.columns, templ)\n",
    "                        N_CLUSTERS = clustered_series.values.max()\n",
    "\n",
    "                    no_found_pairs = (counts * (counts - 1) / 2).sum()\n",
    "\n",
    "                    working_analytic_dict['n_clusters'] = N_CLUSTERS\n",
    "                    working_analytic_dict['clust_algo'] = CLUST_ALGO \n",
    "                    working_analytic_dict['distance_measure'] = DISTANCE \n",
    "\n",
    "                    infomax_scores = InfoMax().get_entropies(assigned_labels)\n",
    "                    working_analytic_dict['infomax_hofs'] = infomax_scores[0]\n",
    "                    working_analytic_dict['infomax_hofk'] = infomax_scores[1]\n",
    "                    working_analytic_dict['no_found_pairs_from_clustering'] = no_found_pairs\n",
    "\n",
    "\n",
    "                    if (no_found_pairs <= 500):\n",
    "\n",
    "                        pairs_unsupervised, unique_tickers, coint_pval_counts = mp_series_analyser.mp_apply_check_properties(pair_results_cache=pair_results_cache, \n",
    "                                                                                                                             clustered_series=clustered_series, \n",
    "                                                                                                                             ex_args=ex_args)\n",
    "\n",
    "                        for _pair in pairs_unsupervised:\n",
    "                            if not mp_series_analyser.check_if_cached(pair_results_cache, _pair[0], _pair[1]):\n",
    "                                pair_results_cache.append(_pair)\n",
    "\n",
    "                        flat_pvals = np.array(flatten(coint_pval_counts))\n",
    "                        \n",
    "                        if len(flat_pvals) > 3:\n",
    "\n",
    "                            significant_pairs_unsupervised = correct_pairs_fdr([valid_pair for valid_pair in pairs_unsupervised if valid_pair[2] != None],\n",
    "                                                                               flat_pvals, q=Q_VAL_THRESHOLD)\n",
    "\n",
    "                            if len(significant_pairs_unsupervised) > 1:\n",
    "\n",
    "                                mp_trader = lfl.backtester.MPTrader(rand_seed)\n",
    "\n",
    "                                mp_trader.df_prices_train = df_prices_train\n",
    "                                mp_trader.df_prices_test = df_prices_test\n",
    "                                \n",
    "                                train_results_with_costs, performance_threshold_train = \\\n",
    "                                      mp_trader.mp_apply_trading_strategy_with_costs(significant_pairs_unsupervised, \n",
    "                                                                      2,#entry_multiplier,\n",
    "                                                                      0,#exit_multiplier,\n",
    "                                                                      test_mode=False,\n",
    "                                                                      train_val_split=train_val_split\n",
    "                                                                    )\n",
    "\n",
    "                                insample_statistics = mp_trader.get_results(train_results_with_costs, performance_threshold_train,\n",
    "                                                                significant_pairs_unsupervised, ticker_segment_dict, n_years_val, 'insample')\n",
    "\n",
    "                                saved_returns_file = save_portfolio_returns(performance_threshold_train, significant_pairs_unsupervised, './backtest_results/return_series/')\n",
    "                                insample_statistics['portfolio_returns_saved_file_insample'] = saved_returns_file\n",
    "\n",
    "                                working_analytic_dict.update(insample_statistics)\n",
    "                                \n",
    "                                working_analytic_dict['spreads_saved_file_insample'] = save_spread_returns(performance_threshold_train)\n",
    "\n",
    "                                del train_results_with_costs\n",
    "                                del performance_threshold_train\n",
    "\n",
    "                                # intraday\n",
    "                                n_years_test = round(len(df_prices_test)/(240*78))\n",
    "\n",
    "                                results_with_costs, performance_threshold_test = \\\n",
    "                                      mp_trader.mp_apply_trading_strategy_with_costs(significant_pairs_unsupervised, \n",
    "                                                                      2,#entry_multiplier,\n",
    "                                                                      0,#exit_multiplier,\n",
    "                                                                      test_mode=True,\n",
    "                                                                      train_val_split=train_val_split\n",
    "                                                                    )\n",
    "\n",
    "                                oosample_statistics = mp_trader.get_results(results_with_costs, performance_threshold_test,\n",
    "                                                                significant_pairs_unsupervised, ticker_segment_dict, n_years_val, 'oosample')\n",
    "\n",
    "                                saved_returns_file = save_portfolio_returns(performance_threshold_test, significant_pairs_unsupervised, './backtest_results/return_series/')\n",
    "                                oosample_statistics['portfolio_returns_saved_file_oosample'] = saved_returns_file\n",
    "\n",
    "                                working_analytic_dict.update(oosample_statistics)\n",
    "                                \n",
    "                                working_analytic_dict['spreads_saved_file_oosample'] = save_spread_returns(performance_threshold_test)\n",
    "\n",
    "                                del results_with_costs\n",
    "                                del performance_threshold_test\n",
    "\n",
    "                                # del significant_pairs_unsupervised\n",
    "                                # del pairs_unsupervised\n",
    "\n",
    "                                coint_tests_statistics = postfix_keys_to_dict(dict(pd.Series(flat_pvals).describe()), 'coint_pvals_dist')\n",
    "\n",
    "                                working_analytic_dict.update(coint_tests_statistics)\n",
    "                                \n",
    "                    row_frame = pd.DataFrame.from_dict(working_analytic_dict, orient='index').T\n",
    "\n",
    "                    global_frame = pd.concat([global_frame, row_frame])\n",
    "                                \n",
    "    global_frame.to_csv('./backtest_results/' + get_current_time_hash() + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ti4GDlB2Izb9",
    "outputId": "609aacb1-0243-40c8-b7da-a0788a98d27d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlWz//IBYglO0rpjnxWmmz",
   "collapsed_sections": [],
   "name": "thesis_backtest_gen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
