{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3.8 install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.layers import Input, Dense, Layer, InputSpec, Lambda, Add, Multiply, LeakyReLU, ReLU\n",
    "from tensorflow.keras import regularizers, activations, initializers, constraints, Sequential, layers, optimizers\n",
    "from tensorflow.keras.regularizers import l1_l2 #l2, l1\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.constraints import UnitNorm, Constraint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback, ModelCheckpoint, CallbackList\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import scipy.io as scio\n",
    "from scipy.linalg import orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./logs/hparam_tuning_0\n",
    "# !mkdir ./logs/\n",
    "\n",
    "# # !rm -rf ./checkpoints/\n",
    "# # !mkdir ./checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 116 tickers\n",
      "Total of 116 tickers after removing tickers with Nan values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pairs_trading_package.pairs_trading_backtester import (\n",
    "    SeriesAnalyser, DataProcessor\n",
    ")\n",
    "\n",
    "series_analyser = SeriesAnalyser()\n",
    "data_processor = DataProcessor()\n",
    "\n",
    "# intraday\n",
    "df_prices = pd.read_pickle('../data_folder/original/commodity_ETFs_from_2014_complete.pickle')\n",
    "\n",
    "SPLIT_IDX = 2\n",
    "\n",
    "splits = [ \n",
    "    [('01-01-2012', '31-12-2014'), ('01-01-2015', '31-12-2015'), '2014-01-01'],\n",
    "    [('01-01-2013', '31-12-2015'), ('01-01-2016', '31-12-2016'), '2015-01-01'],\n",
    "    [('01-01-2014', '31-12-2016'), ('01-01-2017', '31-12-2017'), '2016-01-01'] \n",
    "]\n",
    "\n",
    "# split data in training and test\n",
    "df_prices_train, df_prices_test = data_processor.split_data(df_prices, splits[SPLIT_IDX][0], splits[SPLIT_IDX][1], remove_nan=True)\n",
    "\n",
    "df_train_returns = data_processor.get_return_series(df_prices_train)\n",
    "\n",
    "df_training_set = df_train_returns[:int(len(df_train_returns)*0.7)] \n",
    "df_test_set = df_train_returns[int(len(df_train_returns)*0.7):len(df_train_returns)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = len(df_training_set.columns)\n",
    "epochs = 1000\n",
    "\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.RealInterval(10.0, INPUT_SHAPE*1.0))\n",
    "HP_L2 = hp.HParam('l2', hp.RealInterval(0.001, 0.5))\n",
    "HP_NOISE_FACTOR = hp.HParam('noise_factor', hp.RealInterval(0.1, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))\n",
    "HP_DEPTH = hp.HParam('depth', hp.Discrete([1, 3, 6]))\n",
    "\n",
    "METRIC_1 = 'train_loss'\n",
    "METRIC_2 = 'test_loss'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning_' + str(SPLIT_IDX)).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_BATCH_SIZE, HP_L2, HP_NOISE_FACTOR, HP_OPTIMIZER, HP_DEPTH],\n",
    "        metrics=[hp.Metric(METRIC_1, display_name='Train Loss'),\n",
    "                 hp.Metric(METRIC_2, display_name='Test Loss')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X_cov = pd.DataFrame(np.dot(df_training_set.T, df_training_set))\n",
    "refinement_X_cov = pd.DataFrame(np.dot(df_test_set.T, df_test_set)) \n",
    "\n",
    "cov_scaler = StandardScaler()\n",
    "cov_scaler.fit(train_X_cov)\n",
    "\n",
    "train_X_cov = pd.DataFrame(cov_scaler.transform(train_X_cov))\n",
    "refinement_X_cov = pd.DataFrame(cov_scaler.transform(refinement_X_cov))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Layer, InputSpec, Dense, Dropout, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.constraints import UnitNorm, Constraint\n",
    "\n",
    "def func_api_get_autoencoder(dims, embedding_dim=10, act='relu', kernel_regularizer=None):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    if dims != 1:\n",
    "        # internal layers in encoder\n",
    "        for i in range(n_stacks):\n",
    "            h = Dense(dims[i + 1], activation=act, kernel_regularizer=kernel_regularizer, name='encoder_%d' % i)(h)\n",
    "            h = BatchNormalization()(h)\n",
    "\n",
    "    # hidden layer # hidden layer, features are extracted from here\n",
    "    h = Dense(embedding_dim, activation=act, kernel_regularizer=kernel_regularizer, name='embedding')(h)\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_regularizer=kernel_regularizer, name='decoder_%d' % i)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], activation=act, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, run_dir, summary_writer):    \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_X_cov, train_X_cov))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=25).batch(hparams[HP_BATCH_SIZE], drop_remainder=True)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((refinement_X_cov, refinement_X_cov))\n",
    "    test_dataset = test_dataset.shuffle(buffer_size=25).batch(hparams[HP_BATCH_SIZE], drop_remainder=True)\n",
    "    \n",
    "    \n",
    "    logs = {}    \n",
    "    verbose = False\n",
    "    loss_per_epoch = []\n",
    "    noise_factor = hparams[HP_NOISE_FACTOR]\n",
    "    \n",
    "    loss_fn = tf.keras.losses.MeanSquaredError('auto')\n",
    "\n",
    "    train_loss_tracker = tf.keras.metrics.MeanSquaredError('train_loss')\n",
    "    val_loss_tracker = tf.keras.metrics.MeanSquaredError(name='val_loss')\n",
    "    \n",
    "    model = func_api_get_autoencoder([INPUT_SHAPE//depth_factor for depth_factor in range(1, hparams[HP_DEPTH]+1)], \n",
    "                                     kernel_regularizer=l2(hparams[HP_L2]))\n",
    "                                     #l1_l2(l1=hparams[HP_L1], l2=hparams[HP_L2]))    \n",
    "    \n",
    "    if hparams[HP_OPTIMIZER] == 'adam':\n",
    "        optimizer = Adam()\n",
    "        \n",
    "    elif hparams[HP_OPTIMIZER] == 'sgd':\n",
    "        optimizer = SGD()\n",
    "        \n",
    "    elif hparams[HP_OPTIMIZER] == 'rmsprop':\n",
    "        optimizer = RMSprop()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_loss_tracker.update_state(y, logits)\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(x, y):\n",
    "\n",
    "        val_logits = model(x, training=False)\n",
    "        val_loss_tracker.update_state(y, val_logits)\n",
    "\n",
    "    \n",
    "    _callbacks = [EarlyStopping(monitor='val_loss', mode='min', patience=100, restore_best_weights=True),\n",
    "                  ModelCheckpoint(verbose=0, filepath=run_dir + '.ckpt', save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True)]\n",
    "    \n",
    "    callbacks = CallbackList(_callbacks, add_history=True, model=model)\n",
    "\n",
    "    callbacks.on_train_begin(logs=logs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        callbacks.on_epoch_begin(epoch, logs=logs)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            callbacks.on_batch_begin(step, logs=logs)\n",
    "            callbacks.on_train_batch_begin(step, logs=logs)\n",
    "        \n",
    "            # Add noise to the input to feed to Denoising encoder model.\n",
    "            x_noisy_train = x_batch_train + noise_factor*tf.random.normal(shape=tf.shape(x_batch_train), mean=0.0, stddev=1.0, dtype=tf.float64) \n",
    "            x_noisy_train = tf.clip_by_value(x_noisy_train, 0.0, 1.0)\n",
    "\n",
    "            loss_value = train_step(x_noisy_train, y_batch_train)\n",
    "            \n",
    "            callbacks.on_train_batch_end(step, logs=logs)\n",
    "            callbacks.on_batch_end(step, logs=logs)\n",
    "\n",
    "        \n",
    "        tf.summary.scalar(METRIC_1, train_loss_tracker.result(), step=epoch)\n",
    "            \n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_loss = train_loss_tracker.result()        \n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training acc over epoch: %.4f\" % (float(train_loss),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_loss_tracker.reset_states()           \n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(test_dataset):\n",
    "            callbacks.on_batch_begin(step, logs=logs)\n",
    "            callbacks.on_test_batch_begin(step, logs=logs)\n",
    "\n",
    "            # Add noise to the input to feed to Denoising encoder model.\n",
    "            x_noisy_val = x_batch_val + noise_factor*tf.random.normal(shape=tf.shape(x_batch_val), mean=0.0, stddev=1.0, dtype=tf.float64) \n",
    "            x_noisy_val = tf.clip_by_value(x_noisy_val, 0.0, 1.0)\n",
    "\n",
    "            test_step(x_noisy_val, y_batch_val)\n",
    "            \n",
    "            callbacks.on_test_batch_end(step, logs=logs)\n",
    "            callbacks.on_batch_end(step, logs=logs)\n",
    "        \n",
    "        tf.summary.scalar(METRIC_2, val_loss_tracker.result(), step=epoch)\n",
    "\n",
    "        val_loss = val_loss_tracker.result()\n",
    "            \n",
    "        val_loss_tracker.reset_states()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Validation acc: %.4f\" % (float(val_loss),))\n",
    "\n",
    "        loss_per_epoch.append([train_loss, val_loss])\n",
    "\n",
    "        logs['val_loss'] = val_loss\n",
    "\n",
    "        callbacks.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "        if model.stop_training:\n",
    "            break\n",
    "\n",
    "    callbacks.on_train_end(logs=logs)\n",
    "\n",
    "    return loss_fn(train_X_cov, model.predict(train_X_cov)).numpy()\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default() as summary_writer:\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        train_test_model(hparams, run_dir, summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session_num = 0\n",
    "\n",
    "for batch_size in np.random.randint(HP_BATCH_SIZE.domain.min_value, HP_BATCH_SIZE.domain.max_value, 3):\n",
    "    for l2_param in np.random.uniform(HP_L2.domain.min_value, HP_L2.domain.max_value, 3):\n",
    "        for noise_factor in np.random.uniform(HP_NOISE_FACTOR.domain.min_value, HP_NOISE_FACTOR.domain.max_value, 3):\n",
    "            for depth in HP_DEPTH.domain.values:\n",
    "                for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_BATCH_SIZE: batch_size,\n",
    "                        HP_L2: l2_param,\n",
    "                        HP_NOISE_FACTOR: noise_factor,\n",
    "                        HP_OPTIMIZER: optimizer,\n",
    "                        HP_DEPTH: depth,\n",
    "                    }\n",
    "\n",
    "                    run_name = \"run-%d\" % session_num\n",
    "                    print('--- Starting trial: %s' % run_name)\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('logs/hparam_tuning_' + str(SPLIT_IDX) + '/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /tmp/.tensorboard-info/\n",
    "# !rm /tmp/.tensorboard-info/pid-21090.info\n",
    "# %tensorboard --logdir logs/hparam_tuning_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
